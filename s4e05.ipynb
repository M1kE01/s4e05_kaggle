{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TcMsVEEL5GyY",
        "teKPm8Oc5Cil"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### First approach\n"
      ],
      "metadata": {
        "id": "TcMsVEEL5GyY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdtQscWyHTdE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pandas.plotting import bootstrap_plot\n",
        "import plotly.express as px\n",
        "\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "import catboost\n",
        "import xgboost\n",
        "from catboost import CatBoostRegressor\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score,make_scorer\n",
        "from xgboost import XGBRegressor"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv('/kaggle/input/playground-series-s4e5/train.csv')\n",
        "test_data = pd.read_csv('/kaggle/input/playground-series-s4e5/test.csv')\n",
        "submission_data = pd.read_csv('/kaggle/input/playground-series-s4e5/sample_submission.csv')"
      ],
      "metadata": {
        "id": "pYEX40QNIa2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(submission_data.shape)\n",
        "\n",
        "data = train_data.drop('id',axis=1)"
      ],
      "metadata": {
        "id": "2SRHCZEGC6vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe().T"
      ],
      "metadata": {
        "id": "3fJCuM85C9Ad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.kdeplot(data['FloodProbability'], fill=True,gridsize=100)\n",
        "plt.title('FloodProbability')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mUh-ncTiVh0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features = data.keys()"
      ],
      "metadata": {
        "id": "Zxg3bLAEVkwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=7, ncols=3, figsize=(15, 25))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(features):\n",
        "    sns.violinplot(y=data[feature], ax=axes[i])\n",
        "    axes[i].set_title(feature)\n",
        "    axes[i].set_xlabel('')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oDsL_TBGVlvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(nrows=7, ncols=3, figsize=(15, 25))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for i, feature in enumerate(features):\n",
        "    sns.boxplot(y=data[feature], ax=axes[i])\n",
        "    ax=axes[i].set_title(feature)\n",
        "    ax=axes[i].set_xlabel('')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0RLUYzpNVnId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.barplot(x='MonsoonIntensity', y='FloodProbability', data=data)"
      ],
      "metadata": {
        "id": "-3K5ItxYVnKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25, 25))\n",
        "sns.heatmap(data.corr(), annot=True, cmap='coolwarm')\n",
        "plt.title('Correlation Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3L7tvHuB5yw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data ,_ = train_test_split(data,test_size=0.8,random_state=42)"
      ],
      "metadata": {
        "id": "Sle7DOAP5z8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.scatter_3d(Data,x='RiverManagement',y='WetlandLoss',z='AgriculturalPractices',color='FloodProbability')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "GibEMIzK51YI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols = len(data.columns)\n",
        "num_rows = int(np.ceil(num_cols / 3))  #ensure that there are enough rows to accommodate all columns\n",
        "\n",
        "fig, axs = plt.subplots(num_rows, 3, figsize=(20, num_rows * 5))\n",
        "for i, col in enumerate(data.columns):\n",
        "    ax = axs[i // 3, i % 3]\n",
        "    ax.hist(data[col], bins=10)\n",
        "    ax.set_title(f'Histogram of {col}')\n",
        "    ax.set_xlabel('Value')\n",
        "    ax.set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WJIuGoD854B6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(data['FloodProbability'], kde=True)\n",
        "plt.title('Distribution of Flood Probability')\n",
        "plt.xlabel('Flood Probability')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, linestyle=':', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "G4pHcugK54zQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(25, 10))\n",
        "sns.boxplot(data=data[features])\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Features Boxplot',fontsize = 20)\n",
        "plt.ylabel('Frequencies',fontsize = 20)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KHSPIn3KeOfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "subset_features = ['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
        "                   'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality']\n",
        "\n",
        "sns.pairplot(Data[subset_features])\n",
        "plt.title('Pairplot of Selected Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Au00ObFZeQ_Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Data[subset_features].plot(subplots = True)\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VidPlhGbeU36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_two_plots(data):\n",
        "\n",
        "    num_features = len(data.columns)\n",
        "    num_cols = 2\n",
        "    num_rows = (num_features + 1) // num_cols\n",
        "\n",
        "    print(\"Number of features:\", num_features)\n",
        "\n",
        "    fig, axs = plt.subplots(num_rows, num_cols, figsize=(15, num_rows*5))\n",
        "    axs = axs.flatten()\n",
        "\n",
        "    for i, feature in enumerate(data.columns):\n",
        "        try:\n",
        "            sns.violinplot(y=data[feature], ax=axs[i*2])\n",
        "            axs[i*2].set_title('Violinplot of ' + feature)\n",
        "            sns.histplot(data=data, x=feature, kde=True, ax=axs[i*2+1])\n",
        "            axs[i*2+1].set_title('Histogram of ' + feature)\n",
        "        except IndexError:\n",
        "            pass\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "-3Fdbnp3eY4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean_values = data.mean()\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(x=mean_values.index, y=mean_values.values)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title('Mean Value of Features')\n",
        "plt.ylabel('Mean Value')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lT34UbbVeb6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bootstrap_plot(data[feature], size=50, samples=50000, color='blue')"
      ],
      "metadata": {
        "id": "CfhqbyPKqibJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['fsum'] = train_data.iloc[:, :-2].sum(axis=1)\n",
        "test_data['fsum'] = test_data.iloc[:, :-1].sum(axis=1)"
      ],
      "metadata": {
        "id": "_UV2FY-Jqif5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X= train_data.drop(['FloodProbability'], axis=1)\n",
        "y= train_data['FloodProbability']"
      ],
      "metadata": {
        "id": "jd_XXC3kqxc3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = test_data.drop('id',axis=1)"
      ],
      "metadata": {
        "id": "fKJXs2fO4byM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "aXpRtgnY4kI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat = CatBoostRegressor(random_seed=12,\n",
        "                        iterations=1500,\n",
        "                        depth=7,\n",
        "                        colsample_bylevel=1.0,\n",
        "                        verbose=False)"
      ],
      "metadata": {
        "id": "LHcrpUwl4nQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cat.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "YGCEJj5n4oie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = cat.predict(test_data)"
      ],
      "metadata": {
        "id": "N671Tnji4p9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission_data.keys()"
      ],
      "metadata": {
        "id": "EeSTiIrf4uRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id = submission_data['id']"
      ],
      "metadata": {
        "id": "x6Qe6OPP4vsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.DataFrame({'id':id, 'FloodProbability': prediction})\n",
        "submission.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "1gt8E1hR4z9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission"
      ],
      "metadata": {
        "id": "IruBj-DJ41CN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Second approach"
      ],
      "metadata": {
        "id": "teKPm8Oc5Cil"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install autogluon.tabular\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from autogluon.tabular import TabularDataset, TabularPredictor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import r2_score"
      ],
      "metadata": {
        "id": "o8_b7iSy5FYT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee08e01b-1529-46b5-ad8f-35939a38b217"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogluon.tabular\n",
            "  Downloading autogluon.tabular-1.1.0-py3-none-any.whl (308 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.5/308.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<1.29,>=1.21 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular) (1.25.2)\n",
            "Requirement already satisfied: scipy<1.13,>=1.5.4 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular) (1.11.4)\n",
            "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular) (2.0.3)\n",
            "Collecting scikit-learn<1.4.1,>=1.3.0 (from autogluon.tabular)\n",
            "  Downloading scikit_learn-1.4.0-1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.10/dist-packages (from autogluon.tabular) (3.3)\n",
            "Collecting autogluon.core==1.1.0 (from autogluon.tabular)\n",
            "  Downloading autogluon.core-1.1.0-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.features==1.1.0 (from autogluon.tabular)\n",
            "  Downloading autogluon.features-1.1.0-py3-none-any.whl (63 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.0/63.0 kB\u001b[0m \u001b[31m858.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.0->autogluon.tabular) (4.66.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.0->autogluon.tabular) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from autogluon.core==1.1.0->autogluon.tabular) (3.7.1)\n",
            "Collecting boto3<2,>=1.10 (from autogluon.core==1.1.0->autogluon.tabular)\n",
            "  Downloading boto3-1.34.113-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m632.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting autogluon.common==1.1.0 (from autogluon.core==1.1.0->autogluon.tabular)\n",
            "  Downloading autogluon.common-1.1.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil<6,>=5.7.3 in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.0->autogluon.core==1.1.0->autogluon.tabular) (5.9.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from autogluon.common==1.1.0->autogluon.core==1.1.0->autogluon.tabular) (67.7.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.tabular) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.4.1,>=1.3.0->autogluon.tabular) (3.5.0)\n",
            "Collecting botocore<1.35.0,>=1.34.113 (from boto3<2,>=1.10->autogluon.core==1.1.0->autogluon.tabular)\n",
            "  Downloading botocore-1.34.113-py3-none-any.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m30.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3<2,>=1.10->autogluon.core==1.1.0->autogluon.tabular)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<2,>=1.10->autogluon.core==1.1.0->autogluon.tabular)\n",
            "  Downloading s3transfer-0.10.1-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.2/82.2 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=2.0.0->autogluon.tabular) (1.16.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (4.51.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->autogluon.core==1.1.0->autogluon.tabular) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.0->autogluon.tabular) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.0->autogluon.tabular) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.0->autogluon.tabular) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->autogluon.core==1.1.0->autogluon.tabular) (2024.2.2)\n",
            "Installing collected packages: jmespath, scikit-learn, botocore, s3transfer, boto3, autogluon.common, autogluon.features, autogluon.core, autogluon.tabular\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed autogluon.common-1.1.0 autogluon.core-1.1.0 autogluon.features-1.1.0 autogluon.tabular-1.1.0 boto3-1.34.113 botocore-1.34.113 jmespath-1.0.1 s3transfer-0.10.1 scikit-learn-1.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_url_train = '/kaggle/input/playground-series-s4e5/train.csv'\n",
        "train_data = TabularDataset(data_url_train)\n",
        "train_data.head()"
      ],
      "metadata": {
        "id": "Dkt8_o3nxmKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_url_test = '/kaggle/input/playground-series-s4e5/test.csv'\n",
        "test_data = TabularDataset(data_url_test)\n",
        "test_data.head()"
      ],
      "metadata": {
        "id": "AQxzdds6xqVH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_features = list(test_data.drop(columns=[\"id\"]).columns)\n",
        "initial_features"
      ],
      "metadata": {
        "id": "8P1v2FjfxtVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique_vals = []\n",
        "for df in [train_data, test_data]:\n",
        "    for col in initial_features:\n",
        "        unique_vals += list(df[col].unique())\n",
        "\n",
        "unique_vals = list(set(unique_vals))\n",
        "unique_vals"
      ],
      "metadata": {
        "id": "ewZt8t18xwQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for df in [train_data, test_data]:\n",
        "    df['fsum'] = df[initial_features].sum(axis=1)\n",
        "    df['fstd'] = df[initial_features].std(axis=1)\n",
        "    df['special1'] = df['fsum'].isin(np.arange(72, 76))\n",
        "    df['fskew'] = df[initial_features].skew(axis=1)\n",
        "    df['fkurtosis'] = df[initial_features].kurtosis(axis=1)\n",
        "\n",
        "    for i in [0.0, 0.2, 0.4, 0.5, 0.6, 0.8, 1.0]:\n",
        "        df['q_{}'.format(int(i*100))] = df[initial_features].quantile(i, axis = 1)\n",
        "\n",
        "    for v in unique_vals:\n",
        "        df['cnt_{}'.format(v)] = (df[initial_features] == v).sum(axis=1)"
      ],
      "metadata": {
        "id": "dJRmeBb7x3dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = train_test_split(\n",
        "     train_data.drop(columns=[\"id\"]), test_size=0.1, random_state=42, stratify=train_data.FloodProbability)"
      ],
      "metadata": {
        "id": "D9FxuzoMx6LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameter_tune_kwargs = {\n",
        "    'num_trials': 40,\n",
        "    'scheduler' : 'local',\n",
        "    'searcher'  : 'auto',\n",
        "}\n",
        "\n",
        "predictor = TabularPredictor(label = 'FloodProbability',\n",
        "                             eval_metric = 'r2',\n",
        "                             problem_type = \"regression\",\n",
        "                            )\n",
        "predictor.fit(X_train,\n",
        "              time_limit = 11*60*60,\n",
        "              hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
        "              presets = 'good_quality',\n",
        "              save_space = True,\n",
        "              keep_only_best = False,\n",
        "             )"
      ],
      "metadata": {
        "id": "5auZmL5px7cP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictor.evaluate(X_test)"
      ],
      "metadata": {
        "id": "5Gm4fr0iyB2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LB = predictor.leaderboard(X_test)\n",
        "LB"
      ],
      "metadata": {
        "id": "jzvU8Hj4yDeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_preds = predictor.predict(test_data.drop(columns=[\"id\"]) )\n",
        "submission = pd.read_csv(\"/kaggle/input/playground-series-s4e5/sample_submission.csv\")\n",
        "submission.FloodProbability = test_preds.values\n",
        "submission.head()"
      ],
      "metadata": {
        "id": "2YmdAKStyErc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv(\"submission.csv\",index=False)"
      ],
      "metadata": {
        "id": "GkZYr_D4yF-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Third approach"
      ],
      "metadata": {
        "id": "UjuXJn8O2Hqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np, pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_squared_log_error\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import KFold\n",
        "from xgboost import XGBRegressor\n",
        "from catboost import CatBoostRegressor"
      ],
      "metadata": {
        "id": "NzMe6IHU2Jkv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train=pd.read_csv(\"/kaggle/input/playground-series-s4e5/train.csv\")\n",
        "df_test=pd.read_csv(\"/kaggle/input/playground-series-s4e5/test.csv\")\n",
        "#\n",
        "print(\"Train:\",len(df_train))\n",
        "sample_sub=pd.read_csv(\"/kaggle/input/playground-series-s4e5/sample_submission.csv\")\n",
        "#\n",
        "df_train.head()"
      ],
      "metadata": {
        "id": "juyH4RnT3WG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_cols=['MonsoonIntensity', 'TopographyDrainage', 'RiverManagement',\n",
        "       'Deforestation', 'Urbanization', 'ClimateChange', 'DamsQuality',\n",
        "       'Siltation', 'AgriculturalPractices', 'Encroachments',\n",
        "       'IneffectiveDisasterPreparedness', 'DrainageSystems',\n",
        "       'CoastalVulnerability', 'Landslides', 'Watersheds',\n",
        "       'DeterioratingInfrastructure', 'PopulationScore', 'WetlandLoss',\n",
        "       'InadequatePlanning', 'PoliticalFactors']\n",
        "\n",
        "unique_vals = []\n",
        "for df in [df_train, df_test]:\n",
        "    for col in num_cols:\n",
        "        unique_vals += list(df[col].unique())\n",
        "\n",
        "unique_vals = list(set(unique_vals))\n",
        "#\n",
        "def getFeats(df):\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    df['ClimateAnthropogenicInteraction'] = (df['MonsoonIntensity'] + df['ClimateChange']) * (df['Deforestation'] + df['Urbanization'] + df['AgriculturalPractices'] + df['Encroachments'])\n",
        "    df['InfrastructurePreventionInteraction'] = (df['DamsQuality'] + df['DrainageSystems'] + df['DeterioratingInfrastructure']) * (df['RiverManagement'] + df['IneffectiveDisasterPreparedness'] + df['InadequatePlanning'])\n",
        "\n",
        "    df['sum'] = df[num_cols].sum(axis=1)\n",
        "    df['std']  = df[num_cols].std(axis=1)\n",
        "    df['mean'] = df[num_cols].mean(axis=1)\n",
        "    df['max']  = df[num_cols].max(axis=1)\n",
        "    df['min']  = df[num_cols].min(axis=1)\n",
        "    df['mode'] = df[num_cols].mode(axis=1)[0]\n",
        "    df['median'] = df[num_cols].median(axis=1)\n",
        "    df['q_25th'] = df[num_cols].quantile(0.25, axis=1)\n",
        "    df['q_75th'] = df[num_cols].quantile(0.75, axis=1)\n",
        "    df['skew'] = df[num_cols].skew(axis=1)\n",
        "    df['kurt'] = df[num_cols].kurt(axis=1)\n",
        "    df['sum_72_76'] = df['sum'].isin(np.arange(72, 76))\n",
        "    for i in range(10,100,10):\n",
        "        df[f'{i}th'] = df[num_cols].quantile(i/100, axis=1)\n",
        "    df['harmonic'] = len(num_cols) / df[num_cols].apply(lambda x: (1/x).mean(), axis=1)\n",
        "    df['geometric'] = df[num_cols].apply(lambda x: x.prod()**(1/len(x)), axis=1)\n",
        "    df['zscore'] = df[num_cols].apply(lambda x: (x - x.mean()) / x.std(), axis=1).mean(axis=1)\n",
        "    df['cv'] = df['std'] / df['mean']\n",
        "    df['Skewness_75'] = (df[num_cols].quantile(0.75, axis=1) - df[num_cols].mean(axis=1)) / df[num_cols].std(axis=1)\n",
        "    df['Skewness_25'] = (df[num_cols].quantile(0.25, axis=1) - df[num_cols].mean(axis=1)) / df[num_cols].std(axis=1)\n",
        "    df['2ndMoment'] = df[num_cols].apply(lambda x: (x**2).mean(), axis=1)\n",
        "    df['3rdMoment'] = df[num_cols].apply(lambda x: (x**3).mean(), axis=1)\n",
        "    df['entropy'] = df[num_cols].apply(lambda x: -1*(x*np.log(x)).sum(), axis=1)\n",
        "\n",
        "    for v in unique_vals:\n",
        "        if v<16:\n",
        "            df['cnt_{}'.format(v)] = (df[num_cols] == v).sum(axis=1)\n",
        "\n",
        "    df[num_cols] = scaler.fit_transform(df[num_cols])\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "2BopNT_T9Smv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['typ']=0\n",
        "df_test['typ']=1\n",
        "#\n",
        "df_all=pd.concat([df_train,df_test],axis=0)\n",
        "df_all=getFeats(df_all)\n",
        "df_all.head()\n"
      ],
      "metadata": {
        "id": "heYafFpA9W1o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train=df_all[df_all['typ']==0]\n",
        "df_test=df_all[df_all['typ']==1]\n",
        "#\n",
        "X=df_train.drop(['id','FloodProbability','typ'],axis=1)\n",
        "y=df_train['FloodProbability']\n",
        "#\n",
        "feats=list(X.columns)"
      ],
      "metadata": {
        "id": "zw4SQNqi9cjB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lgb_params = {\n",
        "    'boosting_type': 'gbdt',\n",
        "    'n_estimators':2000,\n",
        "    'learning_rate' :  0.012,\n",
        "    #'device':'gpu',\n",
        "    'num_leaves' : 250,\n",
        "    'subsample_for_bin': 165700,\n",
        "    'min_child_samples': 114,\n",
        "    'reg_alpha': 2.075e-06,\n",
        "    'reg_lambda': 3.839e-07,\n",
        "    'colsample_bytree': 0.9634,\n",
        "    'subsample': 0.9592,\n",
        "    'max_depth': 10,\n",
        "    'random_state':0,\n",
        "    'verbosity':-1}\n",
        "\n",
        "xgb_params ={'n_estimators':8000,\n",
        "             'max_depth': 10,\n",
        "             #'tree_method': 'gpu_hist',\n",
        "             'learning_rate': 0.01,\n",
        "             'random_state':0,\n",
        "             }\n",
        "\n",
        "\n",
        "cat_params = {'n_estimators':12000,\n",
        "             'l2_leaf_reg': 0.0017992898021052064,\n",
        "             'max_bin': 200,\n",
        "             'learning_rate': 0.016714889518285515,\n",
        "             'max_depth': 7,\n",
        "             'random_state': 0,\n",
        "             'min_data_in_leaf': 288\n",
        "             }"
      ],
      "metadata": {
        "id": "OmmhEeAIduSj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Function for Cross Validation\n",
        "def cross_val_train(X,y,df_test,params,mName):\n",
        "\n",
        "    spl=7\n",
        "    test_preds = np.zeros((len(df_test)))\n",
        "    val_preds = np.zeros((len(X)))\n",
        "    val_scores, train_scores = [],[]\n",
        "\n",
        "    cv = KFold(spl, shuffle=True, random_state=42)\n",
        "\n",
        "    for fold, (train_ind, valid_ind) in enumerate(cv.split(X,y)):\n",
        "\n",
        "        X_train = X.iloc[train_ind]\n",
        "        y_train = y[train_ind]\n",
        "        X_val = X.iloc[valid_ind]\n",
        "        y_val = y[valid_ind]\n",
        "\n",
        "        if mName=='LGB':\n",
        "            model = lgb.LGBMRegressor(**params)\n",
        "            model.fit(X_train, y_train,\n",
        "                        eval_set=[(X_val, y_val)],\n",
        "                        callbacks=[lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(100)])\n",
        "\n",
        "        if mName=='XGB':\n",
        "            model = XGBRegressor(**params)\n",
        "            model.fit(X_train, y_train,\n",
        "                              eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "                              early_stopping_rounds=50,\n",
        "                              verbose=100)\n",
        "\n",
        "        if mName=='CAT':\n",
        "            model = CatBoostRegressor(**params)\n",
        "            model.fit(X_train, y_train,\n",
        "                              eval_set=[(X_train, y_train), (X_val, y_val)],\n",
        "                              early_stopping_rounds=50,\n",
        "                              verbose=500)\n",
        "\n",
        "        y_pred_trn=model.predict(X_train)\n",
        "        y_pred_val=model.predict(X_val)\n",
        "        train_r2 = r2_score(y_train, y_pred_trn)\n",
        "        val_r2 = r2_score(y_val, y_pred_val)\n",
        "        print(\"Fold:\",fold, \" Train R2:\",np.round(train_r2,5), \" Val R2:\",np.round(val_r2,5))\n",
        "\n",
        "        test_preds += model.predict(df_test[feats])/spl\n",
        "        val_preds[valid_ind] = model.predict(X_val)\n",
        "        val_scores.append(val_r2)\n",
        "        print(\"-\"*50)\n",
        "\n",
        "    return val_scores, val_preds, test_preds\n",
        "\n",
        "# Evaluate the model\n",
        "def modelEval(y,val_preds):\n",
        "    mse = mean_squared_error(y,val_preds)\n",
        "    rmse = np.sqrt(mean_squared_error(y, val_preds))\n",
        "    r2 = r2_score(y, val_preds)\n",
        "    #\n",
        "    print(f'MSE: {mse}')\n",
        "    print(f'RMSE: {rmse}')\n",
        "    print(f'R2: {r2}')\n"
      ],
      "metadata": {
        "id": "xAEwdYC2duje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_scores_cat, val_preds_cat, test_preds_cat=cross_val_train(X,y,df_test,cat_params,'CAT')"
      ],
      "metadata": {
        "id": "4UF-NE3pkJBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "modelEval(y,val_preds_cat)"
      ],
      "metadata": {
        "id": "PRxk1cU6kLHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_scores_lgb, val_preds_lgb, test_preds_lgb=cross_val_train(X,y,df_test,lgb_params,'LGB')"
      ],
      "metadata": {
        "id": "mcqZo3KYkNQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "modelEval(y,val_preds_lgb)"
      ],
      "metadata": {
        "id": "zMpN6Dk7kPaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_scores_xgb, val_preds_xgb, test_preds_xgb=cross_val_train(X,y,df_test,xgb_params,'XGB')\n"
      ],
      "metadata": {
        "id": "n1_PSTdlkRLk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "modelEval(y,val_preds_xgb)"
      ],
      "metadata": {
        "id": "oiJhvSP_kS8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_preds = val_preds_lgb*0.6 + val_preds_xgb*0.3 + val_preds_cat*0.1\n",
        "test_preds = test_preds_lgb*0.6 + test_preds_xgb*0.3 + test_preds_cat*0.1"
      ],
      "metadata": {
        "id": "mKPErp_IkVoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the Ensemble\n",
        "modelEval(y,val_preds)"
      ],
      "metadata": {
        "id": "lJFMMbcqkhL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "# Calculate residuals\n",
        "residuals = y - val_preds\n",
        "# Plot residuals\n",
        "plt.scatter(val_preds, residuals)\n",
        "plt.axhline(y=0, color='red', linestyle='--',linewidth=3)\n",
        "plt.xlabel('Predicted Values')\n",
        "plt.ylabel('Residuals')\n",
        "plt.title('Residual Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "QhGRbEAgkil5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot histogram of residuals\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.hist(residuals, bins=100, edgecolor='black')\n",
        "plt.xlabel('Residuals')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Histogram of Residuals')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yBKjzWr8klJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub=sample_sub[['id']]\n",
        "sub['FloodProbability'] = test_preds\n",
        "sub.to_csv('submission.csv', index=False)\n",
        "sub.head()"
      ],
      "metadata": {
        "id": "MYAQtt70kmze"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}